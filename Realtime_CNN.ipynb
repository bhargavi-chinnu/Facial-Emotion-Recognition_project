{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMzsETIottX+C9FfAtCzA9+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install opencv-python-headless\n","!pip install fer\n","!pip install opencv-python\n","!pip install deepface\n"],"metadata":{"id":"MaiK9icaTArF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","import cv2\n","import numpy as np\n","import base64  # Import base64 for decoding image data\n","from deepface import DeepFace\n","\n","# JavaScript code to capture the image from the webcam\n","def take_photo(filename='photo.jpg', quality=0.8):\n","    js = Javascript('''\n","    async function takePhoto(quality) {\n","        const div = document.createElement('div');\n","        const capture = document.createElement('button');\n","        capture.textContent = 'Capture Image';\n","        div.appendChild(capture);\n","\n","        const video = document.createElement('video');\n","        video.style.display = 'block';\n","        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        // Resize the output to match the video element.\n","        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","        // Wait for the user to click the capture button.\n","        await new Promise((resolve) => capture.onclick = resolve);\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        canvas.getContext('2d').drawImage(video, 0, 0);\n","        stream.getTracks()[0].stop();\n","\n","        const dataUrl = canvas.toDataURL('image/jpeg', quality);\n","        div.remove();\n","        return dataUrl;\n","    }\n","    ''')\n","    display(js)\n","    data = eval_js('takePhoto(%f)' % quality)\n","    binary = np.frombuffer(base64.b64decode(data.split(',')[1]), dtype=np.uint8)\n","    image = cv2.imdecode(binary, cv2.IMREAD_COLOR)\n","    return image\n","\n","# Take photo using webcam\n","image = take_photo()\n","\n","# Show the captured image\n","cv2_imshow(image)\n","\n","# Analyze emotions using DeepFace\n","try:\n","    result = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n","\n","    # Access the first element in the list of results if multiple faces are detected\n","    if isinstance(result, list):\n","        result = result[0]\n","\n","    print(\"Detected Emotion: \", result['dominant_emotion'])\n","except Exception as e:\n","    print(f\"Error analyzing emotion: {str(e)}\")\n"],"metadata":{"id":"c-Cg6GhYV3tB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","import cv2\n","import numpy as np\n","import base64  # Import base64 for decoding image data\n","from deepface import DeepFace\n","import time\n","\n","# Global flag to stop the loop\n","stop_loop = False\n","\n","# JavaScript code to capture the image from the webcam and stop the loop\n","def take_photo_or_stop(filename='photo.jpg', quality=0.8):\n","    js = Javascript('''\n","    async function takePhotoOrStop(quality) {\n","        const div = document.createElement('div');\n","\n","        // Capture Image button\n","        const capture = document.createElement('button');\n","        capture.textContent = 'Capture Image';\n","        div.appendChild(capture);\n","\n","        // Stop button\n","        const stopButton = document.createElement('button');\n","        stopButton.textContent = 'Stop';\n","        div.appendChild(stopButton);\n","\n","        const video = document.createElement('video');\n","        video.style.display = 'block';\n","        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        // Resize the output to match the video element.\n","        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","        // Capture Image button functionality\n","        await new Promise((resolve, reject) => {\n","            capture.onclick = resolve;\n","            stopButton.onclick = reject;  // Reject if Stop button is clicked\n","        });\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        canvas.getContext('2d').drawImage(video, 0, 0);\n","        stream.getTracks()[0].stop();\n","\n","        const dataUrl = canvas.toDataURL('image/jpeg', quality);\n","        div.remove();\n","        return [true, dataUrl];  // Image captured\n","    }\n","    ''')\n","\n","    try:\n","        display(js)\n","        data = eval_js('takePhotoOrStop(%f)' % quality)\n","        return data\n","    except Exception:\n","        return [False, None]  # Loop stop requested\n","\n","\n","# Simulated real-time emotion detection with Stop button\n","while True:\n","    # Capture image or stop the loop\n","    capture_status, dataUrl = take_photo_or_stop()\n","\n","    if not capture_status:\n","        print(\"Stopping the real-time emotion detection loop.\")\n","        break\n","\n","    # Decode and process the captured image\n","    binary = np.frombuffer(base64.b64decode(dataUrl.split(',')[1]), dtype=np.uint8)\n","    image = cv2.imdecode(binary, cv2.IMREAD_COLOR)\n","\n","    # Show the captured image\n","    cv2_imshow(image)\n","\n","    # Analyze emotions using DeepFace\n","    try:\n","        result = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n","\n","        # Access the first element in the list of results if multiple faces are detected\n","        if isinstance(result, list):\n","            result = result[0]\n","\n","        print(\"Detected Emotion: \", result['dominant_emotion'])\n","    except Exception as e:\n","        print(f\"Error analyzing emotion: {str(e)}\")\n","\n","    # Simulate real-time delay (adjust time.sleep as needed)\n","    time.sleep(1)\n"],"metadata":{"id":"mvr1QudLZo5e"},"execution_count":null,"outputs":[]}]}